behaviors:
  RocketLanding:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024 # number of experiences used per training update
      buffer_size: 20480 # number of experiences stored in the replay buffer; the larger the buffer, the stable but slower the training
      learning_rate: 3.0e-4 # step size for updating the model parameters; the smaller the learning rate, the slower but more stable the training
      beta: 1.0e-3 # strength of the entropy regularization term; the larger the beta, the more exploration
      epsilon: 0.2 # Clipping parameter for PPO; controls how much the policy can change during an update
      lambd: 0.95 # (0-1) the higher, the less biased but more variable the estimate of the advantage function
      learning_rate_schedule: linear # learning rate schedule; options: constant, linear, exponential Note: decrease the learning rate over time to stabilize training
      num_epoch:    3 # number of times the training loop is run per update
    network_settings:
      normalize: true # normalize the input states
      hidden_units: 1024 # number of units in the hidden layers (originally 256)
      num_layers: 5 # number of hidden layers (originally 3)
    # reward_signals:
    #   extrinsic:
    #     gamma: 0.99 # (0-1) discount factor for future rewards; the larger the gamma, the more importance the agent places on future rewards
    #     strength: 1.0 # (0-1) scales the reward signal; the larger the strength, the more the agent relies on the reward signal
    #   gail:
    #     strength: 0.1 # (0-1) scales the reward signal; the larger the strength, the more the agent relies on the reward signal
    #     demo_path: "demos/newlanding.demo"
    # behavioral_cloning:
    #   demo_path: "demos/newlanding.demo"
      strength: 0.1 # (0-1) scales the reward signal; the larger the strength, the more the agent relies on the reward signal
    max_steps: 750000 # maximum number of steps in a given training session
    time_horizon: 128 # number of steps the agent considers before making a decision
    summary_freq: 20000 # how often to save training statistics

# HOW TO USE THIS FILE
# mlagents-learn config/config.yaml --run-id=MoveToGoal (--force, --train, --resume)
# visual stats: tensorboard --logdir results

# [INFO] Hyperparameters for behavior name RocketLanding:
#         trainer_type:   ppo
#         hyperparameters:
#           batch_size:   1024
#           buffer_size:  10240
#           learning_rate:        0.0003
#           beta: 0.005
#           epsilon:      0.2
#           lambd:        0.95
#           num_epoch:    3
#           shared_critic:        False
#           learning_rate_schedule:       linear
#           beta_schedule:        linear
#           epsilon_schedule:     linear
#         checkpoint_interval:    500000
#         network_settings:
#           normalize:    False
#           hidden_units: 128
#           num_layers:   2
#           vis_encode_type:      simple
#           memory:       None
#           goal_conditioning_type:       hyper
#           deterministic:        False
#         reward_signals:
#           extrinsic:
#             gamma:      0.99
#             strength:   1.0
#             network_settings:
#               normalize:        False
#               hidden_units:     128
#               num_layers:       2
#               vis_encode_type:  simple
#               memory:   None
#               goal_conditioning_type:   hyper
#               deterministic:    False
#         init_path:      None
#         keep_checkpoints:       5
#         even_checkpoints:       False
#         max_steps:      500000
#         time_horizon:   64
#         summary_freq:   50000
#         threaded:       False
#         self_play:      None
#         behavioral_cloning:     None